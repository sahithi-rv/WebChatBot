from langchain_community.vectorstores import Chroma
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from text_to_doc import get_doc_chunks
from web_crawler import get_data_from_website
from prompt import get_prompt
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain.prompts import PromptTemplate
from langchain_groq import ChatGroq

def get_chroma_client():
    """
    Returns a chroma vector store instance.

    Returns:
        langchain.vectorstores.chroma.Chroma: ChromaDB vector store instance.
    """
    embedding = FastEmbedEmbeddings()
    return Chroma(
        collection_name="website_data",
        embedding_function=embedding,
        persist_directory="data/chroma")


def store_docs(url):
    """
    Retrieves data from a website, processes it into document chunks, and stores them in a vector store.

    Args:
        url (str): The URL of the website to retrieve data from.

    Returns:
        None
    """
    text, metadata = get_data_from_website(url)
    docs = get_doc_chunks(text, metadata)
    vector_store = get_chroma_client()
    vector_store.add_documents(docs)
    vector_store.persist()


def make_chain():
    """
    Creates a chain of langchain components.

    Returns:
        langchain.chains.ConversationalRetrievalChain: ConversationalRetrievalChain instance.
    """
    
    """
    model = ChatOpenAI(
            model_name="gpt-3.5-turbo",
            temperature=0.0,
            verbose=True
        )
    """
    llm = ChatGroq(
        groq_api_key="unknown"
        model_name="llama3-8b-8192"  # Or another appropriate Groq model
    )
    vector_store = get_chroma_client()
    #prompt = get_prompt()
    raw_prompt = PromptTemplate.from_template(
        """
        [INST] 
        The user asked '{input}'.  Based on the retrieved documents and context {context}, please provide a comprehensive and informative answer to their question.
        [/INST] 
        """
    )

    retriever = vector_store.as_retriever(search_type="mmr", verbose=True)

    document_chain = create_stuff_documents_chain(llm, raw_prompt)
    chain = create_retrieval_chain(retriever, document_chain)
    #chain = ConversationalRetrievalChain.from_llm(
    #    llm,
    #    retriever=retriever,
    #    return_source_documents=True,
    #    combine_docs_chain_kwargs=dict(prompt=prompt),
    #    verbose=True,
    #    rephrase_question=False,
    #)
    return chain


def get_response(question):
    """
    Generates a response based on the input question.

    Args:
        question (str): The input question to generate a response for.
        organization_name (str): The name of the organization.
        organization_info (str): Information about the organization.
        contact_info (str): Contact information for the organization.

    Returns:
        str: The response generated by the chain model.
    """
    chat_history = ""
    chain = make_chain()
    response = chain({"input": question, "chat_history": chat_history})
    return response['answer']
